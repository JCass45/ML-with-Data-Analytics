{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import pyspark.mllib.regression \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "NUM_FEATURES = 13\n",
    "NUM_DATAPOINTS = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spark_context():\n",
    "    \"\"\"\n",
    "    Creates a spark creates a spark context\n",
    "    Package dependencies: pyspark.SparkContext\n",
    "    Input: None\n",
    "    Returns: sc - SparkContext object\n",
    "    \"\"\"\n",
    "    conf = (SparkConf()\n",
    "            .setMaster('local')\n",
    "            .setAppName('RfClassifier')\n",
    "            .set(\"spark.executor.memory\", \"2g\"))\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = create_spark_context()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "with open('kc_house_data.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i, row in enumerate(reader):         \n",
    "        if(i != 0):\n",
    "            xi = [None]*13\n",
    "            xi[0] = row[3]\n",
    "            xi[1] = row[4]\n",
    "            xi[2] = row[5]\n",
    "            xi[3] = row[6]\n",
    "            xi[4] = row[7]\n",
    "            xi[5] = row[8]\n",
    "            xi[6] = row[9]\n",
    "            xi[7] = row[10]\n",
    "            xi[8] = row[11]\n",
    "            xi[9] = row[12]\n",
    "            xi[10] = row[13]\n",
    "            xi[11] = row[14]\n",
    "            xi[12] = row[15]\n",
    "            #y.append(str(int(float(row[2])>530000)))\n",
    "            y.append(row[2])\n",
    "            X.append(xi)\n",
    "                \n",
    "            \n",
    "        if(i == NUM_DATAPOINTS):\n",
    "            break\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, yi in enumerate(y):\n",
    "    xi = X[i]\n",
    "    xi_string = \"\"\n",
    "    xi_string+=str(yi)\n",
    "    for x in xi:\n",
    "        xi_string+=','\n",
    "        xi_string+=str(x)\n",
    "        \n",
    "        \n",
    "    \n",
    "    data.append(xi_string)\n",
    "\n",
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = rdd.map(lambda line: Row(price=line[0],\n",
    "                              bedrooms=line[1], \n",
    "                              bathrooms=line[2], \n",
    "                              sqft_living=line[3],\n",
    "                              sqft_lot=line[4], \n",
    "                              floors=line[5], \n",
    "                              waterfront=line[6],\n",
    "                              view=line[7], \n",
    "                              condition=line[8], \n",
    "                              grade=line[9],\n",
    "                              sqft_above=line[10], \n",
    "                              sqft_basement=line[11], \n",
    "                              yr_built=line[12],\n",
    "                              yr_renovated=line[13])).toDF()\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([3.0, 3.0, 1.0, 7.0, 221900.0, 1180.0, 0.0, 1180.0, 5650.0, 0.0, 0.0, 1955.0, 0.0]), features_scaled=DenseVector([3.275, 4.5025, 1.9536, 6.0042, 0.5889, 1.4552, 0.0, 1.295, 0.1254, 0.0, 0.0, 69.8507, 0.0])),\n",
       " Row(label=2.25, features=DenseVector([3.0, 3.0, 2.0, 7.0, 538000.0, 2170.0, 400.0, 2570.0, 7242.0, 0.0, 0.0, 1951.0, 1991.0]), features_scaled=DenseVector([3.275, 4.5025, 3.9072, 6.0042, 1.4278, 2.6761, 0.8872, 2.8205, 0.1608, 0.0, 0.0, 69.7077, 4.7653]))]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([.7,.3],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.6478412610750475, 0.0),\n",
       " (1.9515305444098665, 0.0),\n",
       " (2.941477850227784, 0.0),\n",
       " (0.8366467027631117, 0.75),\n",
       " (0.9956107865808672, 0.75)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4198623388850159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7074292319832243"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(linearModel.summary.rootMeanSquaredError)\n",
    "\n",
    "# Get the R2\n",
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
