{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import pyspark.mllib.regression \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from sklearn import preprocessing\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = 13\n",
    "NUM_DATAPOINTS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spark_context():\n",
    "    \"\"\"Set up spark context\"\"\"\n",
    "    conf = (SparkConf())\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create spark context\n",
    "sc = create_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_unscaled = []\n",
    "y_class = []\n",
    "y_reg_unscaled = []\n",
    "#Extract data from csv file\n",
    "with open(cf.PATHS[cf.HOUSE_PRICES]) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i, row in enumerate(reader):         \n",
    "        if(i != 0):\n",
    "            xi = [None]*13\n",
    "            xi[0] = row[3]\n",
    "            xi[1] = row[4]\n",
    "            xi[2] = row[5]\n",
    "            xi[3] = row[6]\n",
    "            xi[4] = row[7]\n",
    "            xi[5] = row[8]\n",
    "            xi[6] = row[9]\n",
    "            xi[7] = row[10]\n",
    "            xi[8] = row[11]\n",
    "            xi[9] = row[12]\n",
    "            xi[10] = row[13]\n",
    "            xi[11] = row[14]\n",
    "            xi[12] = row[15]\n",
    "            y_reg_unscaled.append(row[2])\n",
    "            y_class.append(str(int(float(row[2])>530000)))\n",
    "            X_unscaled.append(xi)\n",
    "                \n",
    "        if(i == NUM_DATAPOINTS):\n",
    "            break\n",
    "\n",
    "X = preprocessing.scale(X_unscaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinReg:\n",
    "    \"\"\"A class that takes in a numpy matrix X and column vector y \n",
    "        Allows you to apply 7030 split and 10 fold cross fold validation\n",
    "        with metrics returned as a tuple as so:\n",
    "        (RMSE, MAE)\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.spark = SparkSession(sc)\n",
    "        self.X = X\n",
    "        self.y = preprocessing.scale(y)\n",
    "        self.data = []\n",
    "        for i, yi in enumerate(self.y):\n",
    "            xi = self.X[i]\n",
    "            xi_string = \"\"\n",
    "            xi_string+=str(yi)\n",
    "            for x in xi:\n",
    "                xi_string+=','\n",
    "                xi_string+=str(x)\n",
    "            self.data.append(xi_string)\n",
    "\n",
    "        self.rdd = sc.parallelize(self.data)\n",
    "        self.rdd = self.rdd.map(lambda line: line.split(\",\"))\n",
    "        #Convert rdd to df for easier manip\n",
    "        self.df = self.rdd.map(lambda line: Row(price=line[0],\n",
    "                              bedrooms=line[1], \n",
    "                              bathrooms=line[2], \n",
    "                              sqft_living=line[3],\n",
    "                              sqft_lot=line[4], \n",
    "                              floors=line[5], \n",
    "                              waterfront=line[6],\n",
    "                              view=line[7], \n",
    "                              condition=line[8], \n",
    "                              grade=line[9],\n",
    "                              sqft_above=line[10], \n",
    "                              sqft_basement=line[11], \n",
    "                              yr_built=line[12],\n",
    "                              yr_renovated=line[13])).toDF()\n",
    "        self.columns = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n",
    "        self.df = self.convertColumn(self.df, self.columns, FloatType())\n",
    "        #Seperate data in labels and features\n",
    "        self.input_data = self.df.rdd.map(lambda x: (x[5], DenseVector(x[:5] + x[6:])))\n",
    "        self.df1 = self.spark.createDataFrame(self.input_data, [\"label\", \"features\"])\n",
    "        \n",
    "    def convertColumn(self, df, names, newType):\n",
    "        \"\"\"Cast columns from string to floats\"\"\"\n",
    "        for name in names: \n",
    "            df = df.withColumn(name, df[name].cast(newType))\n",
    "        return df \n",
    "    \n",
    "    def split7030(self):\n",
    "        \"\"\"Applies 70 30 split testing\"\"\"\n",
    "        train_data, test_data = self.df1.randomSplit([.7,.3],seed=1234)\n",
    "        # Initialize `lr`\n",
    "        lr = LinearRegression(labelCol=\"label\", maxIter=10)\n",
    "\n",
    "        # Fit the data to the model\n",
    "        linear_model = lr.fit(train_data)\n",
    "        linear_model.transform(test_data)\n",
    "        metric_1 = linear_model.summary.rootMeanSquaredError\n",
    "        metric_2 = linear_model.summary.meanAbsoluteError\n",
    "        return metric_1, metric_2\n",
    "    \n",
    "    def crossval10(self):\n",
    "        \"\"\"Applies 10 fold cross validation testing\"\"\"\n",
    "        metric_1_sum = 0\n",
    "        metric_2_sum = 0\n",
    "        for i in range(0, 10):\n",
    "            train_data, test_data = self.df1.randomSplit([.9,.1],seed=i*1234)\n",
    "            self.lr = LinearRegression(labelCol=\"label\", maxIter=10)\n",
    "            self.linear_model = self.lr.fit(train_data)\n",
    "            self.linear_model.transform(test_data)\n",
    "            metric_1 = self.linear_model.summary.rootMeanSquaredError\n",
    "            metric_1_sum += metric_1\n",
    "            metric_2 = self.linear_model.summary.meanAbsoluteError\n",
    "            metric_2_sum += metric_2\n",
    "            \n",
    "            \n",
    "        return metric_1_sum/10, metric_2_sum/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinReg(X, y_reg_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold cross validation\n",
      "Root mean square error:  0.5932805764764201\n",
      "Mean absolute error:  0.37457785145920003\n"
     ]
    }
   ],
   "source": [
    "result = lr.crossval10()\n",
    "print('10 fold cross validation')\n",
    "print(\"Root mean square error: \", result[0])\n",
    "print(\"Mean absolute error: \", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 30 split\n",
      "Root mean square error:  0.5761615380448505\n",
      "Mean absolute error:  0.3785820167943481\n"
     ]
    }
   ],
   "source": [
    "result = lr.split7030()\n",
    "print('70 30 split')\n",
    "print(\"Root mean square error: \", result[0])\n",
    "print(\"Mean absolute error: \", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandF:\n",
    "    \"\"\"A class that takes in a numpy matrix X and column vector y \n",
    "        and implements the random forest algorithm with a max depth of 2\n",
    "        Allows you to apply 7030 split and 10 fold cross fold validation\n",
    "        with metrics returned as a tuple as so:\n",
    "        (f1 score, accuracy)\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.spark = SparkSession(sc)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.data = []\n",
    "        for i, yi in enumerate(self.y):\n",
    "            xi = self.X[i]\n",
    "            xi_string = \"\"\n",
    "            xi_string+=str(yi)\n",
    "            for x in xi:\n",
    "                xi_string+=','\n",
    "                xi_string+=str(x)\n",
    "            self.data.append(xi_string)\n",
    "\n",
    "        self.rdd = sc.parallelize(self.data)\n",
    "        self.rdd = self.rdd.map(lambda line: line.split(\",\"))\n",
    "        self.df = self.rdd.map(lambda line: Row(price=line[0],\n",
    "                              bedrooms=line[1], \n",
    "                              bathrooms=line[2], \n",
    "                              sqft_living=line[3],\n",
    "                              sqft_lot=line[4], \n",
    "                              floors=line[5], \n",
    "                              waterfront=line[6],\n",
    "                              view=line[7], \n",
    "                              condition=line[8], \n",
    "                              grade=line[9],\n",
    "                              sqft_above=line[10], \n",
    "                              sqft_basement=line[11], \n",
    "                              yr_built=line[12],\n",
    "                              yr_renovated=line[13])).toDF()\n",
    "        self.columns = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n",
    "        self.df = self.convertColumn(self.df, self.columns, FloatType())\n",
    "        self.input_data = self.df.rdd.map(lambda x: (x[5], DenseVector(x[:5] + x[6:])))\n",
    "        self.df1 = self.spark.createDataFrame(self.input_data, [\"label\", \"features\"])\n",
    "        \n",
    "    def convertColumn(self, df, names, newType):\n",
    "        \"\"\"Converts columns to floats\"\"\"\n",
    "        for name in names: \n",
    "            df = df.withColumn(name, df[name].cast(newType))\n",
    "        return df \n",
    "    \n",
    "    def split7030(self):\n",
    "        \"\"\"Applies 70 30 split testing\"\"\"\n",
    "        train_data, test_data = self.df1.randomSplit([.7,.3],seed=1234)\n",
    "        rf = RandomForestClassifier(labelCol='label', featuresCol='features',numTrees=3)\n",
    "        fit = rf.fit(train_data)\n",
    "        predicted = fit.transform(test_data)\n",
    "        predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "        labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "        prediction_and_label = predictions.zip(labels).collect()\n",
    "       \n",
    "        return self.metrics(prediction_and_label)\n",
    "        \n",
    "    \n",
    "    def crossval10(self):\n",
    "        \"\"\"Applies 10 fold cross validation testing\"\"\"\n",
    "        metric_1_sum = 0\n",
    "        metric_2_sum = 0\n",
    "        for i in range(0, 10):\n",
    "            train_data, test_data = self.df1.randomSplit([.7,.3],seed=i*1234)\n",
    "\n",
    "            rf = RandomForestClassifier(labelCol='label', featuresCol='features',numTrees=3)\n",
    "            fit = rf.fit(train_data)\n",
    "            predicted = fit.transform(test_data)\n",
    "\n",
    "            predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "            labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "            prediction_and_label = predictions.zip(labels).collect()\n",
    "            m = self.metrics(prediction_and_label)\n",
    "            metric_1_sum += m[0]\n",
    "            metric_2_sum += m[1]\n",
    "            \n",
    "        return metric_1_sum/10, metric_2_sum/10     \n",
    "            \n",
    "    def metrics(self, prediction_and_label):\n",
    "        \"\"\"returns f1 score and accuracy as tuple from an array of (predication, label) tuples\"\"\"\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        for p, l in prediction_and_label:\n",
    "            p_bool = 0\n",
    "            if(p>0.5):\n",
    "                p_bool = 1\n",
    "            if(p_bool == 1 and l ==1):\n",
    "                TP+=1\n",
    "            if(p_bool == 1 and l ==0):\n",
    "                FP+=1\n",
    "            if(p_bool == 0 and l ==0):\n",
    "                TN+=1\n",
    "            if(p_bool == 0 and l ==1):\n",
    "                FN+=1\n",
    "                \n",
    "        accuracy = float(TP+TN)/float(TP+FP+TN+FN)        \n",
    "        recall = float(TP)/float(TP+FN)\n",
    "        precision = float(TP)/float(TP+FP)\n",
    "        f1 = 2*(recall*precision)/(recall + precision)\n",
    "        return f1, accuracy   \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandF(X,y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 30 split\n",
      "f1 score:  0.7487131492746841\n",
      "accuracy:  0.8204613841524574\n"
     ]
    }
   ],
   "source": [
    "result = rf.split7030()\n",
    "print('70 30 split')\n",
    "print('f1 score: ', result[0])\n",
    "print('accuracy: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold cross validation\n",
      "f1 score:  0.7225243060295502\n",
      "accuracy:  0.8133795231525955\n"
     ]
    }
   ],
   "source": [
    "result = rf.crossval10()\n",
    "print('10 fold cross validation')\n",
    "print('f1 score: ', result[0])\n",
    "print('accuracy: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
