{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import pyspark.mllib.regression \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from sklearn import preprocessing\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = 10\n",
    "NUM_DATAPOINTS = 100000\n",
    "CLASSES = {\n",
    "    'Very Large Number': 1,\n",
    "    'Large Number': 1,\n",
    "    'Medium Number': 0,\n",
    "    'Small Number': 0,\n",
    "    'Very Small Number': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spark_context():\n",
    "    \"\"\"Set up spark context\"\"\"\n",
    "    conf = (SparkConf())\n",
    "    sc = SparkContext(conf=conf)\n",
    "\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = create_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_unscaled = []\n",
    "y_class = []\n",
    "y_reg_unscaled = []\n",
    "#Read data in from csv file and store as matrix and column vectors\n",
    "with open(cf.PATHS[cf.SUM_WITH_NOISE]) as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for i, row in enumerate(reader):         \n",
    "        if(i != 0):\n",
    "            row = row[0].split(';')\n",
    "            xi = [None]*NUM_FEATURES\n",
    "            xi[0] = row[1]\n",
    "            xi[1] = row[2]\n",
    "            xi[2] = row[3]\n",
    "            xi[3] = row[4]\n",
    "            xi[4] = row[5]\n",
    "            xi[5] = row[6]\n",
    "            xi[6] = row[7]\n",
    "            xi[7] = row[8]\n",
    "            xi[8] = row[9]\n",
    "            xi[9] = row[10]\n",
    "            y_reg_unscaled.append(row[11])\n",
    "            y_class.append(CLASSES[row[12]])\n",
    "            X_unscaled.append(xi)\n",
    "            \n",
    "        if(i == NUM_DATAPOINTS):\n",
    "            break\n",
    "#Standardise the data\n",
    "X = preprocessing.scale(X_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinReg:\n",
    "    \"\"\"A class that takes in a numpy matrix X and column vector y \n",
    "        Allows you to apply 7030 split and 10 fold cross fold validation\n",
    "        with metrics returned as a tuple as so:\n",
    "        (RMSE, MAE)\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.spark = SparkSession(sc)\n",
    "        self.X = X\n",
    "        #Standardise y values\n",
    "        self.y = preprocessing.scale(y)\n",
    "        self.data = []\n",
    "        #Convert to string for ease of construct for the RDD\n",
    "        for i, yi in enumerate(self.y):\n",
    "            xi = self.X[i]\n",
    "            xi_string = \"\"\n",
    "            xi_string+=str(yi)\n",
    "            for x in xi:\n",
    "                xi_string+=','\n",
    "                xi_string+=str(x)\n",
    "            self.data.append(xi_string)\n",
    "\n",
    "        self.rdd = sc.parallelize(self.data)\n",
    "        self.rdd = self.rdd.map(lambda line: line.split(\",\"))\n",
    "        #Convert to df for easy manipulation\n",
    "        self.df = self.rdd.map(lambda line: Row(target=line[0],\n",
    "                              f1=line[1], \n",
    "                              f2=line[2], \n",
    "                              f3=line[3],\n",
    "                              f4=line[4], \n",
    "                              f5=line[5], \n",
    "                              f6=line[6],\n",
    "                              f7=line[7], \n",
    "                              f8=line[8], \n",
    "                              f9=line[9],\n",
    "                              f10=line[10])).toDF()\n",
    "        self.columns = ['target','f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10']\n",
    "        self.df = self.convertColumn(self.df, self.columns, FloatType())\n",
    "        self.input_data = self.df.rdd.map(lambda x: (x[10], DenseVector(x[:10] )))\n",
    "        self.df1 = self.spark.createDataFrame(self.input_data, [\"label\", \"features\"])\n",
    "        \n",
    "    def convertColumn(self, df, names, newType):\n",
    "        \"\"\"Convert string columns to float columns\"\"\"\n",
    "        for name in names: \n",
    "            df = df.withColumn(name, df[name].cast(newType))\n",
    "        return df \n",
    "    \n",
    "    def split7030(self):\n",
    "        \"\"\"Apply 70 30 split testing\"\"\"\n",
    "        train_data, test_data = self.df1.randomSplit([.7,.3],seed=1234)\n",
    "        # Initialize `lr`\n",
    "        lr = LinearRegression(labelCol=\"label\", maxIter=10)\n",
    "\n",
    "        # Fit the data to the model\n",
    "        linear_model = lr.fit(train_data)\n",
    "        linear_model.transform(test_data)\n",
    "        metric_1 = linear_model.summary.rootMeanSquaredError\n",
    "        metric_2 = linear_model.summary.meanAbsoluteError\n",
    "        return metric_1, metric_2\n",
    "    \n",
    "    def crossval10(self):\n",
    "        \"\"\"Apply 10 fold cross validation testing\"\"\"\n",
    "        metric_1_sum = 0\n",
    "        metric_2_sum = 0\n",
    "        for i in range(0, 10):\n",
    "            train_data, test_data = self.df1.randomSplit([.9,.1],seed=i*1234)\n",
    "            self.lr = LinearRegression(labelCol=\"label\", maxIter=10)\n",
    "            self.linear_model = self.lr.fit(train_data)\n",
    "            self.linear_model.transform(test_data)\n",
    "            metric_1 = self.linear_model.summary.rootMeanSquaredError\n",
    "            metric_1_sum += metric_1\n",
    "            metric_2 = self.linear_model.summary.meanAbsoluteError\n",
    "            metric_2_sum += metric_2\n",
    "            \n",
    "            \n",
    "        return metric_1_sum/10, metric_2_sum/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinReg(X, y_reg_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error:  0.11975642411407696\n",
      "Mean absolute error:  0.08961014980816806\n"
     ]
    }
   ],
   "source": [
    "result = lr.split7030()\n",
    "print(\"Root mean square error: \", result[0])\n",
    "print(\"Mean absolute error: \", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error:  0.11995015298624785\n",
      "Mean absolute error:  0.08981624145365084\n"
     ]
    }
   ],
   "source": [
    "result = lr.crossval10()\n",
    "print(\"Root mean square error: \", result[0])\n",
    "print(\"Mean absolute error: \", result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandF:\n",
    "    \"\"\"A class that takes in a numpy matrix X and column vector y \n",
    "        and implements the random forest algorithm with a max depth of 2\n",
    "        Allows you to apply 7030 split and 10 fold cross fold validation\n",
    "        with metrics returned as a tuple as so:\n",
    "        (f1 score, accuracy)\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.spark = SparkSession(sc)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.data = []\n",
    "        for i, yi in enumerate(self.y):\n",
    "            xi = self.X[i]\n",
    "            xi_string = \"\"\n",
    "            xi_string+=str(yi)\n",
    "            for x in xi:\n",
    "                xi_string+=','\n",
    "                xi_string+=str(x)\n",
    "            self.data.append(xi_string)\n",
    "\n",
    "        self.rdd = sc.parallelize(self.data)\n",
    "        self.rdd = self.rdd.map(lambda line: line.split(\",\"))\n",
    "        self.df = self.rdd.map(lambda line: Row(target=line[0],\n",
    "                              f1=line[1], \n",
    "                              f2=line[2], \n",
    "                              f3=line[3],\n",
    "                              f4=line[4], \n",
    "                              f5=line[5], \n",
    "                              f6=line[6],\n",
    "                              f7=line[7], \n",
    "                              f8=line[8], \n",
    "                              f9=line[9],\n",
    "                              f10=line[10])).toDF()\n",
    "        self.columns = ['target','f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10']\n",
    "        self.df = self.convertColumn(self.df, self.columns, FloatType())\n",
    "        self.input_data = self.df.rdd.map(lambda x: (x[10], DenseVector(x[:10] )))\n",
    "        self.df1 = self.spark.createDataFrame(self.input_data, [\"label\", \"features\"])\n",
    "        \n",
    "    def convertColumn(self, df, names, newType):\n",
    "        for name in names: \n",
    "            df = df.withColumn(name, df[name].cast(newType))\n",
    "        return df \n",
    "    \n",
    "    def split7030(self):\n",
    "        \"\"\"Apply 70 30 split testing\"\"\"\n",
    "        train_data, test_data = self.df1.randomSplit([.7,.3],seed=1234)\n",
    "        rf = RandomForestClassifier(labelCol='label', featuresCol='features',numTrees=3)\n",
    "        fit = rf.fit(train_data)\n",
    "        predicted = fit.transform(test_data)\n",
    "        predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "        labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "        prediction_and_label = predictions.zip(labels).collect()\n",
    "       \n",
    "        return self.metrics(prediction_and_label)\n",
    "        \n",
    "    \n",
    "    def crossval10(self):\n",
    "        \"\"\"Apply 10 fold cross validation\"\"\"\n",
    "        metric_1_sum = 0\n",
    "        metric_2_sum = 0\n",
    "        for i in range(0, 10):\n",
    "            train_data, test_data = self.df1.randomSplit([.9,.1],seed=i*1234)\n",
    "\n",
    "            rf = RandomForestClassifier(labelCol='label', featuresCol='features',numTrees=3)\n",
    "            fit = rf.fit(train_data)\n",
    "            predicted = fit.transform(test_data)\n",
    "\n",
    "            predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "            labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "            prediction_and_label = predictions.zip(labels).collect()\n",
    "            m = self.metrics(prediction_and_label)\n",
    "            metric_1_sum += m[0]\n",
    "            metric_2_sum += m[1]\n",
    "            \n",
    "        return metric_1_sum/10, metric_2_sum/10     \n",
    "            \n",
    "    def metrics(self, prediction_and_label):\n",
    "        \"\"\"returns f1 score and accuracy as tuple from an array of (predication, label) tuples\"\"\"\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        TN = 0\n",
    "        FN = 0\n",
    "        for p, l in prediction_and_label:\n",
    "            p_bool = 0\n",
    "            if(p>0.5):\n",
    "                p_bool = 1\n",
    "            if(p_bool == 1 and l ==1):\n",
    "                TP+=1\n",
    "            if(p_bool == 1 and l ==0):\n",
    "                FP+=1\n",
    "            if(p_bool == 0 and l ==0):\n",
    "                TN+=1\n",
    "            if(p_bool == 0 and l ==1):\n",
    "                FN+=1\n",
    "                \n",
    "        accuracy = float(TP+TN)/float(TP+FP+TN+FN)        \n",
    "        recall = float(TP)/float(TP+FN)\n",
    "        precision = float(TP)/float(TP+FP)\n",
    "        f1 = 2*(recall*precision)/(recall + precision)\n",
    "        return f1, accuracy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandF(X, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 30 split\n",
      "f1 score:  0.9968501523845197\n",
      "accuracy:  0.9938527994683503\n"
     ]
    }
   ],
   "source": [
    "result = rf.split7030()\n",
    "print('70 30 split')\n",
    "print('f1 score: ', result[0])\n",
    "print('accuracy: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold cross validation\n",
      "f1 score:  0.9967683300965937\n",
      "accuracy:  0.9936936602452546\n"
     ]
    }
   ],
   "source": [
    "result = rf.crossval10()\n",
    "print('10 fold cross validation')\n",
    "print('f1 score: ', result[0])\n",
    "print('accuracy: ', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
